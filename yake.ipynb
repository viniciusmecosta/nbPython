{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z-HdwN9zeMF"
      },
      "source": [
        "!pip install yake\n",
        "!pip install matplotlib\n",
        "!pip install spacy\n",
        "!pip install nltk\n",
        "!python -m spacy download pt_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8VnVULf3KX3",
        "outputId": "1fd600c2-d9c7-4bb2-9a56-54712584f357"
      },
      "outputs": [],
      "source": [
        "!pip install yake\n",
        "!pip install matplotlib\n",
        "!pip install spacy\n",
        "!pip install nltk\n",
        "!python -m spacy download pt_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU19bKWucaDw",
        "outputId": "77a2601a-1b37-49a5-cbd0-d1c767e7341b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import yake\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import spacy\n",
        "from collections import Counter\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itUZlFbQcaD6"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(text):\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    stop_words = set(stopwords.words('portuguese'))\n",
        "    stop_words.add(\"user\")\n",
        "    stop_words.add(\"the\")\n",
        "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stop_words and len(word) > 1]\n",
        "    filtered_text = ' '.join(filtered_words)\n",
        "    return filtered_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpXFdHG5caD_"
      },
      "outputs": [],
      "source": [
        "def tokenizar(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYVhFC_gcaEC"
      },
      "outputs": [],
      "source": [
        "def lemmatizar_tokens(tokens):\n",
        "    doc = spacy.tokens.Doc(nlp.vocab, words=tokens)\n",
        "    lemmatized_tokens = [token.lemma_ for token in nlp(doc)]\n",
        "    return lemmatized_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHkbkBcrcaEE"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data_path):\n",
        "    df = pd.read_csv(data_path)\n",
        "    df.dropna(inplace=True)\n",
        "    df['text_processed'] = df['text'].apply(lambda x: remove_stopwords(x))\n",
        "    df['text_tokens'] = df['text_processed'].apply(lambda x: tokenizar(x))\n",
        "    df['text_lemmatized'] = df['text_tokens'].apply(lambda x: lemmatizar_tokens(x))\n",
        "    df['text_final'] = df['text_lemmatized'].apply(lambda tokens: ' '.join(tokens))\n",
        "    return df['text_final'], df['toxic']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bpguo4tWDb4X"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_model(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=none)\n",
        "    vectorizer = CountVectorizer()\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "    clf = MultinomialNB()\n",
        "    clf.fit(X_train_vec, y_train)\n",
        "    y_pred = clf.predict(X_test_vec)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Acur√°cia do modelo:\", accuracy)\n",
        "    return clf, vectorizer, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqFdUkZpzN_g"
      },
      "outputs": [],
      "source": [
        "X, y = preprocess_data('comentarios_toxicos_ptBR.csv')\n",
        "train_and_evaluate_model(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OjFE9VucaEF"
      },
      "outputs": [],
      "source": [
        "def plot_keyword_scores(keywords):\n",
        "    keywords.sort(key=lambda x: x[1])\n",
        "    keywords_list, scores_list = zip(*keywords)\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    plt.bar(keywords_list, scores_list)\n",
        "    plt.xlabel(\"Keywords\")\n",
        "    plt.ylabel(\"Scores\")\n",
        "    plt.title(\"Keyword Scores\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwxvtJDAcaD9"
      },
      "outputs": [],
      "source": [
        "def extract_keywords(data, language='pt', n=1, k=20):\n",
        "    data = ' '.join(data.astype(str))\n",
        "    keyword_extractor = yake.KeywordExtractor(lan=language, n=n, top=k)\n",
        "    keywords = keyword_extractor.extract_keywords(data)\n",
        "    keywords.sort()\n",
        "    return keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQiTro1XzN_i"
      },
      "outputs": [],
      "source": [
        "keywords = extract_keywords(X)\n",
        "for kw in keywords:\n",
        "    print(kw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPN_QpWWcaEG"
      },
      "outputs": [],
      "source": [
        "plot_keyword_scores(keywords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaWjDo_mzN_i"
      },
      "outputs": [],
      "source": [
        "words = [word.lower() for sentence in X for word in sentence.split()]\n",
        "word_freq = Counter(words)\n",
        "most_common_words = word_freq.most_common(20)\n",
        "for m_w in most_common_words:\n",
        "    print(m_w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVSgJTOR1sc6"
      },
      "outputs": [],
      "source": [
        "plot_keyword_scores(most_common_words)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
